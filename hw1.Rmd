---
title: "PSTAT 131 hw1"
author: "Katlyn Shaw"
date: "9/27/2022"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
# load packages
library(tidyverse)
library(tidymodels)
library(ISLR)
library(corrplot)
```

# Machine Learning Main Ideas

## Q1

Supervised learning involves predicting a future response from the input (predictors), whereas in unsupervised learning there is no response, instead the goal is to describe associations and patterns from the input.

## Q2

In regression models, the response (Y) is quantitative and in classification models the response (Y) is qualitative. We'll want to use linear regression when quantitative and logistic regression when qualitative.

## Q3

Two commonly used metrics for regression ML problems are training MSE and test MSE. Two for classification ML are training error rate and test error rate.

## Q4

- Descriptive Models visually emphasizes trend in data
- Inferential Models state relationship between input and response
- Predictive Models predict response with minimum reducible error

## Q5

- Mechanistic/parametric: Assumes a parametric form for $f$ to estimate a set of parameters.
- empirically driven/non-parametric: Makes no explicit assumption about $f$ they seek an estimate of f that gets as close to the data points as possible without being too rough or wiggly.

I think in general a mechanistic model is easier to understand because instead of estimating $f$ you're simplifying the problem by only estimating a set of parameters.

Empirically driven models are more variable and less biased compared to mechanistic models by default. So, you need to consider the bias-variance trade-off when choosing mechanistic or empirically driven models.

## Q6

- Given a voter’s profile/data, how likely is it that they will vote in favor of the candidate?
Predictive. We are predicting Y (response) from input.

- How would a voter’s likelihood of support for the candidate change if they had personal contact with the candidate?
Inferential. We are inferring the relationship between different predictors.

# Exploratory Data Analysis

## E1

```{r}
mpg %>%
  ggplot(aes(x=hwy)) + geom_histogram()

```

It looks like a bimodal distribution with peaks at hwy=16 and hwy=26.

## E2

```{r}
mpg %>%
  ggplot(aes(x=hwy, y=cty)) + geom_point()
```

This plot is linear, there is a positive relationship between hwy and cty. This means that city miles per gallon increases with highway miles per gallon. 

## E3

```{r}
mpg %>%
  ggplot(aes(y=reorder(manufacturer,manufacturer,
                     function(y)-length(y)))) + geom_bar() + ylab("manufacturer")

```

Dodge produced the most cars, Lincoln the least.

## E4

```{r}
mpg %>%
  ggplot(aes(hwy, group=cyl))+geom_boxplot()
```

I notice that as cyl decreases, hwy *generally* increases.

## E5

```{r}
df1 <- data.frame(mpg)
M = cor(df1[,c("displ","year","cyl","cty","hwy")]) # subsetting for only numeric values
corrplot(M, method="number", type="lower")
```

Positive correlations: 

- displ (engine displacement) and year (of manufacture)

- displ and cyl (number of cylinders)

- cyl and year

- hwy (highway miles per gallon) and year

- hwy and cty (city miles per gallon)


Negative correlations:

- cty and displ

- hwy and displ

- cty and year

- cty and cyl

- hwy and cyl


Most of these relationships make sense to me, city and highway mpg should share positive correlation. It surprises me that year doesn't have a strong correlation (positive or negative) with any other feature. 